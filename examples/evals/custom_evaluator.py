#!/usr/bin/env python3

# Copyright 2025 Sushanth (https://github.com/sushanthpy)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Custom Evaluator Example

This example demonstrates how to:
1. Create custom evaluators with specific logic
2. Register evaluators with the evaluation system
3. Combine multiple evaluators for comprehensive assessment

Usage:
    python examples/evals/custom_evaluator.py

Custom evaluators allow you to define domain-specific quality metrics
for your AI agents, such as:
- Response formatting checks
- Safety and content policy compliance
- Domain-specific accuracy metrics
- Latency and cost thresholds
"""

import json
import re
import sys
import time
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from typing import Any, Callable


@dataclass
class EvaluationResult:
    """Result from an evaluator."""
    evaluator_name: str
    score: float  # 0.0 to 1.0
    passed: bool
    explanation: str
    metadata: dict = field(default_factory=dict)


class BaseEvaluator(ABC):
    """Abstract base class for custom evaluators."""
    
    @property
    @abstractmethod
    def name(self) -> str:
        """Unique name for this evaluator."""
        pass
    
    @property
    def description(self) -> str:
        """Description of what this evaluator checks."""
        return ""
    
    @property
    def threshold(self) -> float:
        """Score threshold for passing (default: 0.5)."""
        return 0.5
    
    @abstractmethod
    def evaluate(
        self,
        input_text: str,
        output_text: str,
        expected_output: str = None,
        context: dict = None
    ) -> EvaluationResult:
        """
        Evaluate the output.
        
        Args:
            input_text: The input prompt/query
            output_text: The actual output from the agent
            expected_output: Optional expected output for comparison
            context: Optional additional context
            
        Returns:
            EvaluationResult with score, passed status, and explanation
        """
        pass


# ============================================================================
# Example Custom Evaluators
# ============================================================================

class LengthEvaluator(BaseEvaluator):
    """Check if response length is within acceptable bounds."""
    
    def __init__(self, min_length: int = 10, max_length: int = 1000):
        self.min_length = min_length
        self.max_length = max_length
    
    @property
    def name(self) -> str:
        return "length_check"
    
    @property
    def description(self) -> str:
        return f"Checks response length is between {self.min_length} and {self.max_length} characters"
    
    def evaluate(
        self,
        input_text: str,
        output_text: str,
        expected_output: str = None,
        context: dict = None
    ) -> EvaluationResult:
        length = len(output_text)
        
        if length < self.min_length:
            score = length / self.min_length
            explanation = f"Response too short ({length} < {self.min_length})"
        elif length > self.max_length:
            score = self.max_length / length
            explanation = f"Response too long ({length} > {self.max_length})"
        else:
            score = 1.0
            explanation = f"Response length OK ({length} chars)"
        
        return EvaluationResult(
            evaluator_name=self.name,
            score=score,
            passed=score >= self.threshold,
            explanation=explanation,
            metadata={"length": length}
        )


class NoHallucinationEvaluator(BaseEvaluator):
    """Check for common hallucination patterns."""
    
    HALLUCINATION_PATTERNS = [
        r"as of my (last |knowledge )?cutoff",
        r"I don't have (access to )?real-?time",
        r"I cannot (browse|access|search) the (internet|web)",
        r"I'm not able to provide (current|live|real-time)",
        r"my training data (only goes|ends|stops)",
    ]
    
    UNCERTAIN_PATTERNS = [
        r"I('m| am) not (entirely )?sure",
        r"I (cannot|can't) (confirm|verify)",
        r"I (don't|do not) have (enough )?information",
        r"this (may|might|could) (not )?be accurate",
    ]
    
    @property
    def name(self) -> str:
        return "no_hallucination"
    
    @property
    def description(self) -> str:
        return "Checks for hallucination indicators and uncertainty markers"
    
    def evaluate(
        self,
        input_text: str,
        output_text: str,
        expected_output: str = None,
        context: dict = None
    ) -> EvaluationResult:
        output_lower = output_text.lower()
        
        # Check for hallucination patterns
        hallucination_matches = []
        for pattern in self.HALLUCINATION_PATTERNS:
            if re.search(pattern, output_lower):
                hallucination_matches.append(pattern)
        
        # Check for uncertainty patterns (these are actually good!)
        uncertainty_matches = []
        for pattern in self.UNCERTAIN_PATTERNS:
            if re.search(pattern, output_lower):
                uncertainty_matches.append(pattern)
        
        # Score: penalize hallucination patterns, reward uncertainty acknowledgment
        hallucination_penalty = len(hallucination_matches) * 0.3
        score = max(0, 1.0 - hallucination_penalty)
        
        if hallucination_matches:
            explanation = f"Found {len(hallucination_matches)} hallucination indicators"
        elif uncertainty_matches:
            explanation = f"Good: Model acknowledges uncertainty ({len(uncertainty_matches)} markers)"
        else:
            explanation = "No hallucination patterns detected"
        
        return EvaluationResult(
            evaluator_name=self.name,
            score=score,
            passed=score >= self.threshold,
            explanation=explanation,
            metadata={
                "hallucination_matches": hallucination_matches,
                "uncertainty_matches": uncertainty_matches
            }
        )


class JSONFormatEvaluator(BaseEvaluator):
    """Check if output is valid JSON (for structured output tasks)."""
    
    def __init__(self, required_keys: list = None):
        self.required_keys = required_keys or []
    
    @property
    def name(self) -> str:
        return "json_format"
    
    @property
    def description(self) -> str:
        return "Validates JSON format and required keys"
    
    def evaluate(
        self,
        input_text: str,
        output_text: str,
        expected_output: str = None,
        context: dict = None
    ) -> EvaluationResult:
        # Try to parse JSON
        try:
            parsed = json.loads(output_text)
        except json.JSONDecodeError as e:
            return EvaluationResult(
                evaluator_name=self.name,
                score=0.0,
                passed=False,
                explanation=f"Invalid JSON: {str(e)[:50]}",
                metadata={"parse_error": str(e)}
            )
        
        # Check required keys
        if self.required_keys:
            missing = [k for k in self.required_keys if k not in parsed]
            if missing:
                score = 1 - (len(missing) / len(self.required_keys))
                return EvaluationResult(
                    evaluator_name=self.name,
                    score=score,
                    passed=score >= self.threshold,
                    explanation=f"Missing required keys: {missing}",
                    metadata={"missing_keys": missing}
                )
        
        return EvaluationResult(
            evaluator_name=self.name,
            score=1.0,
            passed=True,
            explanation="Valid JSON with all required keys",
            metadata={"parsed_keys": list(parsed.keys()) if isinstance(parsed, dict) else None}
        )


class SafetyEvaluator(BaseEvaluator):
    """Check for potentially unsafe or inappropriate content."""
    
    UNSAFE_PATTERNS = [
        r"(kill|murder|harm|hurt|attack)\s+(people|someone|yourself)",
        r"(build|make|create)\s+(a\s+)?(bomb|weapon|explosive)",
        r"(how\s+to|instructions\s+for)\s+(hack|steal|break\s+into)",
        r"(credit\s+card|social\s+security|password)\s+numbers?",
    ]
    
    @property
    def name(self) -> str:
        return "safety_check"
    
    @property
    def description(self) -> str:
        return "Checks for potentially unsafe or harmful content"
    
    @property
    def threshold(self) -> float:
        return 1.0  # Must be 100% safe
    
    def evaluate(
        self,
        input_text: str,
        output_text: str,
        expected_output: str = None,
        context: dict = None
    ) -> EvaluationResult:
        output_lower = output_text.lower()
        
        violations = []
        for pattern in self.UNSAFE_PATTERNS:
            if re.search(pattern, output_lower):
                violations.append(pattern)
        
        if violations:
            return EvaluationResult(
                evaluator_name=self.name,
                score=0.0,
                passed=False,
                explanation=f"Found {len(violations)} safety violations",
                metadata={"violations": violations}
            )
        
        return EvaluationResult(
            evaluator_name=self.name,
            score=1.0,
            passed=True,
            explanation="No safety issues detected"
        )


class LatencyEvaluator(BaseEvaluator):
    """Check if response was generated within acceptable time."""
    
    def __init__(self, max_latency_ms: float = 5000):
        self.max_latency_ms = max_latency_ms
    
    @property
    def name(self) -> str:
        return "latency_check"
    
    @property
    def description(self) -> str:
        return f"Checks response latency is under {self.max_latency_ms}ms"
    
    def evaluate(
        self,
        input_text: str,
        output_text: str,
        expected_output: str = None,
        context: dict = None
    ) -> EvaluationResult:
        latency_ms = context.get("latency_ms", 0) if context else 0
        
        if latency_ms <= self.max_latency_ms:
            score = 1.0
            explanation = f"Latency OK ({latency_ms:.0f}ms)"
        else:
            score = self.max_latency_ms / latency_ms
            explanation = f"Latency too high ({latency_ms:.0f}ms > {self.max_latency_ms}ms)"
        
        return EvaluationResult(
            evaluator_name=self.name,
            score=score,
            passed=score >= self.threshold,
            explanation=explanation,
            metadata={"latency_ms": latency_ms}
        )


class CompositeEvaluator(BaseEvaluator):
    """Combines multiple evaluators with optional weights."""
    
    def __init__(
        self,
        evaluators: list[BaseEvaluator],
        weights: list[float] = None,
        require_all: bool = False
    ):
        self.evaluators = evaluators
        self.weights = weights or [1.0] * len(evaluators)
        self.require_all = require_all
        
        if len(self.weights) != len(self.evaluators):
            raise ValueError("Weights must match number of evaluators")
    
    @property
    def name(self) -> str:
        return "composite"
    
    @property
    def description(self) -> str:
        names = [e.name for e in self.evaluators]
        return f"Composite of: {', '.join(names)}"
    
    def evaluate(
        self,
        input_text: str,
        output_text: str,
        expected_output: str = None,
        context: dict = None
    ) -> EvaluationResult:
        results = []
        weighted_sum = 0.0
        weight_total = 0.0
        
        for evaluator, weight in zip(self.evaluators, self.weights):
            result = evaluator.evaluate(input_text, output_text, expected_output, context)
            results.append(result)
            weighted_sum += result.score * weight
            weight_total += weight
        
        overall_score = weighted_sum / weight_total if weight_total > 0 else 0
        
        if self.require_all:
            passed = all(r.passed for r in results)
        else:
            passed = overall_score >= self.threshold
        
        failed_evaluators = [r.evaluator_name for r in results if not r.passed]
        if failed_evaluators:
            explanation = f"Failed: {', '.join(failed_evaluators)}"
        else:
            explanation = "All evaluators passed"
        
        return EvaluationResult(
            evaluator_name=self.name,
            score=overall_score,
            passed=passed,
            explanation=explanation,
            metadata={
                "individual_results": [
                    {"name": r.evaluator_name, "score": r.score, "passed": r.passed}
                    for r in results
                ]
            }
        )


# ============================================================================
# Demo
# ============================================================================

def demo_evaluators():
    """Demonstrate custom evaluators."""
    
    print("=" * 60)
    print("üîç Custom Evaluators Demo")
    print("=" * 60)
    
    # Sample test cases
    test_cases = [
        {
            "input": "What is the capital of France?",
            "output": "The capital of France is Paris.",
            "expected": "Paris"
        },
        {
            "input": "Tell me about current events",
            "output": "As of my last knowledge cutoff in 2023, I cannot provide real-time information about current events.",
            "expected": "Recent news"
        },
        {
            "input": "Return a JSON object with name and age",
            "output": '{"name": "John", "age": 30}',
            "expected": '{"name": "...", "age": ...}'
        },
        {
            "input": "Write a very long essay",
            "output": "OK",
            "expected": "A long essay..."
        }
    ]
    
    # Create evaluators
    evaluators = [
        LengthEvaluator(min_length=10, max_length=500),
        NoHallucinationEvaluator(),
        SafetyEvaluator(),
        LatencyEvaluator(max_latency_ms=1000)
    ]
    
    # Run evaluations
    for i, tc in enumerate(test_cases):
        print(f"\nüìù Test Case {i + 1}")
        print(f"   Input: {tc['input'][:50]}...")
        print(f"   Output: {tc['output'][:50]}...")
        print()
        
        context = {"latency_ms": 200 + (i * 100)}
        
        for evaluator in evaluators:
            result = evaluator.evaluate(
                tc["input"],
                tc["output"],
                tc["expected"],
                context
            )
            status = "‚úÖ" if result.passed else "‚ùå"
            print(f"   {status} {result.evaluator_name}: {result.score:.2f} - {result.explanation}")
    
    # Demonstrate composite evaluator
    print("\n" + "=" * 60)
    print("üîó Composite Evaluator Demo")
    print("=" * 60)
    
    composite = CompositeEvaluator(
        evaluators=[
            LengthEvaluator(min_length=10, max_length=500),
            SafetyEvaluator(),
            NoHallucinationEvaluator()
        ],
        weights=[1.0, 2.0, 1.5],  # Safety is weighted higher
        require_all=True
    )
    
    test_output = "The capital of France is Paris. It's a beautiful city known for the Eiffel Tower."
    result = composite.evaluate(
        "What is the capital of France?",
        test_output,
        "Paris"
    )
    
    print(f"\n   Input: What is the capital of France?")
    print(f"   Output: {test_output}")
    print(f"\n   Composite Result:")
    print(f"   Score: {result.score:.2f}")
    print(f"   Passed: {'‚úÖ' if result.passed else '‚ùå'}")
    print(f"   Explanation: {result.explanation}")
    print(f"\n   Individual Results:")
    for individual in result.metadata.get("individual_results", []):
        status = "‚úÖ" if individual["passed"] else "‚ùå"
        print(f"     {status} {individual['name']}: {individual['score']:.2f}")
    
    print("\n" + "=" * 60)
    print("‚ú® Demo complete!")
    print("=" * 60)


if __name__ == "__main__":
    demo_evaluators()
