# Copyright 2025 Sushanth (https://github.com/sushanthpy)
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Sample Evaluator Plugin
# This plugin demonstrates how to create custom evaluators for Agentreplay

[plugin]
name = "sample-evaluator"
version = "0.1.0"
description = "A sample evaluator plugin that demonstrates how to create custom evaluators for Agentreplay"
author = "Agentreplay Team"
license = "MIT"
homepage = "https://github.com/sochdb/sample-evaluator"
repository = "https://github.com/sochdb/sample-evaluator"
keywords = ["evaluator", "sample", "demo"]
categories = ["evaluators"]

# Minimum Agentreplay version required
agentreplay_version = "0.1.0"

# Plugin capabilities - what permissions this plugin needs
[capabilities]
# Read access to traces and spans
trace_read = true
# Write access to evaluation results
eval_write = true
# Access to network for external API calls (e.g., LLM APIs)
network = false
# Access to environment variables (for API keys)
env_vars = ["OPENAI_API_KEY", "ANTHROPIC_API_KEY"]
# File system access
fs_read = []
fs_write = []

# Dependencies on other plugins (optional)
[dependencies]
# "metrics-core" = ">=0.1.0"

# Lifecycle hooks
[hooks]
# Script to run on plugin installation
on_install = "scripts/install.sh"
# Script to run on plugin uninstall
on_uninstall = "scripts/uninstall.sh"
# Script to run when plugin is enabled
on_enable = "scripts/enable.sh"
# Script to run when plugin is disabled
on_disable = "scripts/disable.sh"

# Entry point for the plugin
[entry]
# Type of plugin: "native" (Rust), "wasm", or "script"
type = "native"
# Main entry point
main = "lib/libsample_evaluator.so"
# Platform-specific binaries (optional)
[entry.platforms]
macos-aarch64 = "lib/libsample_evaluator.dylib"
macos-x86_64 = "lib/libsample_evaluator.dylib"
linux-x86_64 = "lib/libsample_evaluator.so"
windows-x86_64 = "lib/sample_evaluator.dll"

# Evaluators provided by this plugin
[[evaluators]]
name = "sentiment-check"
description = "Checks the sentiment of LLM responses"
# Type: "binary" (pass/fail), "score" (0-1), "categorical"
result_type = "score"

[[evaluators]]
name = "length-check"
description = "Ensures response length is within expected bounds"
result_type = "binary"
# Default configuration
[evaluators.config]
min_length = 10
max_length = 5000

[[evaluators]]
name = "toxicity-filter"
description = "Detects potentially toxic or harmful content in responses"
result_type = "score"

# UI extensions (optional)
[ui]
# Custom dashboard widget
[[ui.widgets]]
name = "sentiment-dashboard"
description = "Dashboard widget showing sentiment trends"
entry = "ui/sentiment-widget.js"
type = "dashboard"

# Custom settings page
[[ui.pages]]
name = "Evaluator Settings"
route = "/plugins/sample-evaluator/settings"
entry = "ui/settings.js"

# CLI commands provided by this plugin
[[commands]]
name = "evaluate"
description = "Run evaluation on a trace"
usage = "agentreplay sample-evaluator evaluate <trace-id>"

[[commands]]
name = "batch-evaluate"
description = "Run batch evaluation on multiple traces"
usage = "agentreplay sample-evaluator batch-evaluate --session <session-id>"
